{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [4] NeRF Network\n",
    "\n",
    "## 1. Positional Encoding\n",
    "\n",
    "아래 코드는 NeRF 논문의 **Positional Encoding** 기법을 파이썬 클래스(`Embedder`)로 구현한 예시입니다.  \n",
    "**목표**는 3차원 벡터(예: 위치 $\\mathbf{p}$ 또는 방향 $\\mathbf{d}$)에 대해, 다음과 같은 형태의 트리그 함수(사인/코사인) 묶음을 만들어 고차원 벡터로 확장하는 것입니다.\n",
    "\n",
    "$$\n",
    "\\gamma(\\mathbf{p})\n",
    "= \\Bigl[\n",
    "\\mathbf{p},\\;\n",
    "\\sin(2^0 \\pi \\mathbf{p}), \\cos(2^0 \\pi \\mathbf{p}),\\;\n",
    "\\sin(2^1 \\pi \\mathbf{p}), \\cos(2^1 \\pi \\mathbf{p}),\\;\n",
    "\\cdots,\\;\n",
    "\\sin(2^{L-1} \\pi \\mathbf{p}), \\cos(2^{L-1} \\pi \\mathbf{p})\n",
    "\\Bigr].\n",
    "$$\n",
    "\n",
    "위 식에서:\n",
    "- $\\mathbf{p}$의 차원이 3 (x,y,z)라면, 각 주파수(frequency)마다 $\\sin, \\cos$ 각각 3차원씩 6차원이 추가됩니다.  \n",
    "- $L$개의 주파수가 있으면 총 $2L \\times 3$ 차원을 얻게 되고, “원본 $\\mathbf{p}$ 포함 여부”(`include_input`)에 따라 추가 3차원이 붙어서 최종 63차원(예: $L=10$일 때) 정도가 됩니다.  \n",
    "아래는 **코드 상세 해설**입니다.\n",
    "\n",
    "### 1) Embedder 클래스\n",
    "**`__init__`:** 인스턴스 생성에 필요한 kwargs를 받아 초기화(입력 차원, 옥타브 수, 주기 함수의 종류 등)\n",
    "\n",
    "**`create_embedding_fn`:** 입력 데이터를 태울 지수 함수와 삼각 함수 리스트를 인스턴스 변수로 생성하여 저장\n",
    "\n",
    "**`embed`:** 실제로 입력 데이터를 받고, 임베딩 함수의 리스트로 구성된 인스턴스 변수에 태워 실제 임베딩 결과값을 반환\n",
    "\n",
    "왜 굳이 create_embedding_fn와 embed를 구분하여 코드를 설계하였는가?\n",
    "1. **초기화 단계 vs. 실행 단계**를 **명확히 분리**  \n",
    "   - `create_embedding_fn`는 “어떤 주파수 대역을 쓸 것인지?”, “사인·코사인 등 어떤 함수를 쓸 것인지?”를 결정하고, 그에 맞는 **람다 함수들을 한 번** 생성해 둡니다.  \n",
    "   - 이후 실제 데이터를 넣을 때(`.embed()` 호출 시), 이미 만들어진 함수를 재사용하여 **빠르게** 인코딩을 수행할 수 있습니다. \n",
    "\n",
    "2. **반복 호출 효율**  \n",
    "   - 만약 매번 입력이 들어올 때마다(예: 매 배치마다) “사인·코사인 함수를 생성·세팅” 한다면 오버헤드가 큽니다.  \n",
    "   - 반면 초기 한 번(`create_embedding_fn`)만 람다들을 생성해두고, `.embed(...)`에서는 “이미 만들어진 함수 리스트”를 순회하기만 하면 되므로 **코드 재사용**과 **효율**이 좋아집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zv0z0_TLyFJj"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Positional encoding (section 5.1)\n",
    "class Embedder:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.kwargs = kwargs # “몇 차원의 입력을 받을지?”, “몇 개의 주파수를 사용할지?”, “어떤 함수를 쓸지?(sin, cos 등)” 등의 설정\n",
    "        self.create_embedding_fn() # 바로 create_embedding_fn()을 호출하여 실제 인코딩 함수를 만든다.\n",
    "        \n",
    "    def create_embedding_fn(self):\n",
    "        embed_fns = [] # 임베딩 할 함수들을 담는 리스트\n",
    "        d = self.kwargs['input_dims'] # 입력 차원. view dir과 카메라 위치 좌표 모두 3차원이다.\n",
    "        out_dim = 0 # 최종 Positional Encoding의 출력 차원을 추적하기 위한 변수로, 초기 값은 0으로 세팅.\n",
    "\n",
    "        if self.kwargs['include_input']: # 단순히 인풋 값을 아웃풋에 추가할 것인지.\n",
    "            embed_fns.append(lambda x : x) # 인코딩 벡터에 인풋값인 X, Y, Z 좌표를 추가해준다. (즉, 60차원이 아닌 63차원으로 return)\n",
    "            out_dim += d # out_dim에 3 더하여 추적 업데이트.\n",
    "\n",
    "        # L과 L-1을 정해준다. (하단의 embed_kwargs 참조)\n",
    "        max_freq = self.kwargs['max_freq_log2'] # = max_freq = L-1\n",
    "        N_freqs = self.kwargs['num_freqs'] # N_freqs = Octave = L\n",
    "\n",
    "        if self.kwargs['log_sampling']: # 로그 샘플링\n",
    "            freq_bands = 2.**torch.linspace(0., max_freq, steps=N_freqs)\n",
    "            # 0부터 max_freq(=L-1)까지 N_freqs개를 2의 제곱에 모두 태우는 것.\n",
    "        else: # 리니어 샘플링\n",
    "            freq_bands = torch.linspace(2.**0, 2.**max_freq, step=N_freqs)\n",
    "            # 0부터 max_freq(L-1)까지 N_freqs개를 단순히 리니어 샘플링 한다.\n",
    "            # 실제 코드는 있으나 사용하진 않음.\n",
    "\n",
    "        # 예: max_freq_log2=9, num_freqs=10 → freq_bands = [1, 2, 4, 8, ..., 512(=2^9)]\n",
    "\n",
    "\n",
    "        # 이제 sin, cos에 태워준다.\n",
    "        for freq in freq_bands:\n",
    "            for p_fn in self.kwargs['periodic_fns']: # 어떤 함수에 태울 것인가? (논문에선 cos, sin 사용)\n",
    "                embed_fns.append(lambda x, p_fn=p_fn, freq=freq : p_fn(x * freq)) # 수식에는 π까지 붙이는데 여기선 그냥 sin(2^n * x), cos(2^n * x)\n",
    "                # cf) lambda (input : output)\n",
    "                out_dim += d\n",
    "\n",
    "        # 이후 아래의 다른 함수에서 접근할 수 있도록 self 변수에 대입\n",
    "        self.embed_fns = embed_fns # 포지션 인코딩에 사용할 람다 함수 리스트 (x, sin, cos, ... , sin, cos)\n",
    "        self.out_dim = out_dim # 포지션 인코딩 된 값의 차원\n",
    "\n",
    "    # embed_fns에 input 데이터를 넣어 임베딩 된 결과를 도출한 뒤, torch.cat으로 묶어서 반환해주는 함수\n",
    "    def embed(self, inputs):\n",
    "        return torch.cat([fn(inputs) for fn in self.embed_fns], -1)\n",
    "        # 여러 개의 람다 함수를 순회하며, 입력벡터 inputs에 대해 각각 사인·코사인을 계산.   \n",
    "        # 마지막 차원(-1)으로 연결(torch.cat), 즉 shape가 (N, out_dim) 형태가 됨.\n",
    "\n",
    "        # 예:\n",
    "        # 입력이 shape (N,3)\n",
    "        # 주파수 대역이 L=10, include_input=True라면,\n",
    "        # 최종 출력은 (N, 63)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) get_embedder\n",
    "이 함수는 위에서 선언한 임베더 클래스의 내부 함수들을 실행시켜 결과를 받아내는 역할을 한다.  \n",
    "이렇게 선언된 클래스에서 인스턴스를 생성하고, 내부 함수를 수행하는 함수를 헬퍼 함수라고도 부른다.  \n",
    "헬퍼 함수는 외부 코드에서 직접 인스턴스를 만들고 일일이 인자들을 세팅하는 과정을 하나의 명령어로 컨트롤할 수 있기 때문에 다양한 변인을 바꿔줘야 하는 실험 세팅에 편리함을 준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위에서 정의한 임베더 클래스에서 객체를 생성한 후, view_o와 view_d를 구분하여 임베딩 해주는 함수.\n",
    "# view_o와 view_d의 옥타브 (L = multires)의 값이 각각 다르다.\n",
    "def get_embedder(multires, i = 0): # multires는 옥타브 수\n",
    "    # 만약 i=-1, 즉 “positional encoding을 쓰지 않겠다”는 상황이면, 항등 함수(nn.Identity())와 출력차원 3을 반환.\n",
    "    if i == -1:\n",
    "        return nn.Identity(), 3 # 그냥 같은 값 return.\n",
    "    \n",
    "    # -1이 아닌 경우\n",
    "    # 임베더 인스턴스를 생성하기 전에, 이것이 생성자(__init__)에서 받는 kwargs를 생성해준다.\n",
    "    embed_kwargs = {\n",
    "                'include_input' : True, # 원래 input 값을 output에 넣을 것이냐? (60 -> 63)\n",
    "                'input_dims' : 3, # input_dim은 view_o와 view_d 모두 3차원으로 동일.\n",
    "                'max_freq_log2' : multires-1, # L - 1\n",
    "                'num_freqs' : multires, # = L (옥타브 수).\n",
    "                'log_sampling' : True, # 지수함수에 태울 것인가\n",
    "                'periodic_fns' : [torch.sin, torch.cos], # 어떤 주기함수에 태울 것인가\n",
    "    }\n",
    "    \n",
    "    embedder_obj = Embedder(**embed_kwargs) # 객체 생성 후 위에서 정의한 kwargs로 초기화\n",
    "    embed = lambda x, eo=embedder_obj : eo.embed(x) # 임베딩 된 결과값\n",
    "    \n",
    "    # embed는 positional encoding 된 값, embedder_obj.out_dim는 예를 들어 63이 나올 것\n",
    "    return embed, embedder_obj.out_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function get_embedder.<locals>.<lambda> at 0x7f026b507ee0>\n",
      "63\n",
      "tensor([0.5134, 0.5193, 0.7209])\n",
      "tensor([ 0.5134,  0.5193,  0.7209,  0.4911,  0.4963,  0.6600,  0.8711,  0.8682,\n",
      "         0.7512,  0.8556,  0.8617,  0.9917,  0.5176,  0.5075,  0.1287,  0.8858,\n",
      "         0.8745,  0.2552, -0.4641, -0.4850, -0.9669, -0.8222, -0.8482, -0.4936,\n",
      "        -0.5691, -0.5296,  0.8697,  0.9359,  0.8985, -0.8585, -0.3521, -0.4390,\n",
      "         0.5127, -0.6592, -0.7889, -0.8804, -0.7520, -0.6145, -0.4742,  0.9914,\n",
      "         0.9696,  0.8350,  0.1310, -0.2447, -0.5503,  0.2597, -0.4746, -0.9190,\n",
      "        -0.9657, -0.8802, -0.3943, -0.5015,  0.8354,  0.7248,  0.8651,  0.5496,\n",
      "        -0.6890, -0.8678,  0.9183, -0.9987,  0.4970, -0.3959, -0.0505])\n"
     ]
    }
   ],
   "source": [
    "# 카메라 좌표 (X, Y, Z)를 옥타브 10에 태워 임베딩 하는 예제(torch.rand)\n",
    "embed, out_dim = get_embedder(10,0) # 옥타브 10\n",
    "# embed: 임베딩 함수\n",
    "# outdim: 임베딩 차원수\n",
    "print(embed)\n",
    "print(out_dim)\n",
    "\n",
    "input = torch.rand(3)\n",
    "# embed 함수로 인스턴스 생성부터 결과값\n",
    "output = embed(input) # 처음 3개는 같고, 그 다음 60개는 sin, cos을 태운 것.\n",
    "\n",
    "print(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "zQK50lXuKUt-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function get_embedder.<locals>.<lambda> at 0x7f026b511ca0>\n",
      "63\n",
      "tensor([0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
      "        1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
      "        1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
      "        1., 1., 1., 0., 0., 0., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# 카메라 좌표 (X, Y, Z)를 옥타브 10에 태워 임베딩 하는 예제(torch.zeros)\n",
    "embed, out_dim = get_embedder(10,0) # 옥타브 10\n",
    "print(embed)\n",
    "print(out_dim)\n",
    "input = torch.zeros(3)\n",
    "output = embed(input)\n",
    "\n",
    "print(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function get_embedder.<locals>.<lambda> at 0x7f026b507af0>\n",
      "27\n",
      "tensor([0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
      "        1., 1., 1., 0., 0., 0., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# view_dirs를 옥타브 4에 태워 임베딩 하는 예제\n",
    "embed, out_dim = get_embedder(4, 0) # 뷰 디렉션의 옥타브는 4\n",
    "print(embed)\n",
    "print(out_dim)\n",
    "\n",
    "output = embed(input)\n",
    "\n",
    "print(input)\n",
    "print(output)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. NeRF 네트워크 구현\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "- 처음 인풋 데이터: 시선의 시작점(view_o)  \n",
    "- 5번째 레이어에서 좌표값을 한 번 더 넣어 정보 희석 방지 \n",
    "- 마지막 레이어에서 해당 픽셀의 투명도를 산출  \n",
    "- 마지막 레이어의 인풋으로 시선의 방향(view_d)과 시선의 시작점(view_o) 인풋\n",
    "- 시선 방향에 대한 아웃풋으로 RGB값을 산출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class NeRF(nn.Module):\n",
    "    def __init__(self, D=8, W=256, input_ch=3, input_ch_views=3, output_ch=4, skips=[4], use_viewdirs=False): # output_ch=4인 이유는 rgb와 alpha의 4개이므로.\n",
    "        super(NeRF, self).__init__()\n",
    "        # 생성자로 받은 인자를 클래스 내부 변수로 저장\n",
    "        self.D = D # 네트워크의 총 레이어\n",
    "        self.W = W # 은닉 레이어 너비(채널 수)\n",
    "        self.input_ch = input_ch # 뷰 위치 차원\n",
    "        self.input_ch_views = input_ch_views # 뷰 디렉션 차원\n",
    "        self.skips = skips # 스킵 커넥션 레이어\n",
    "        self.use_viewdirs = use_viewdirs # 뷰 디렉션 사용 여부 (사용)\n",
    "\n",
    "        # 위치 좌표의 입력을 처리하는 레이어\n",
    "        self.pts_linears = nn.ModuleList([nn.Linear(input_ch, W)] +\n",
    "                                         [nn.Linear(W, W) if i not in self.skips else nn.Linear(W+input_ch, W) for i in range(D-1)])\n",
    "        # 스킵 레이어가 아니라면 단순히 W -> W 레이어\n",
    "        # 스킵 레이어라면 W + input_ch -> W 레이어\n",
    "        # 스킵 커넥션의 성능: 깊은 레이어에서도 초기 입력 좌표의 고주파 정보를 계속 사용할 수 있게 됩니다.\n",
    "\n",
    "        # 시점 방향 입력을 처리하는 레이어\n",
    "        self.views_linears = nn.ModuleList([nn.Linear(input_ch_views + W, W//2)])\n",
    "        # W + input_ch_view -> W 레이어\n",
    "\n",
    "        if use_viewdirs: # view_dirs를 사용한다면\n",
    "            self.feature_linear = nn.Linear(W, W) # W -> W 레이어\n",
    "            self.alpha_linear = nn.Linear(W, 1) # 하나의 스칼라 값인 알파값을 뽑기 위한 1채널\n",
    "            self.rgb_linear = nn.Linear(W//2,3) # rgb output을 뽑기 위한 3채널\n",
    "        else: # view_dirs를 사용하지 않는다면\n",
    "            # 단일 레이어 (W -> 4)로 RGB값과 알파 값을 같이 추출\n",
    "            self.output_linear = nn.Linear(W, output_ch) # rgb 알파까지 해서 4채널\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input x에는 좌표에 대한 인코딩 값인 self.input_ch와 view dir에 대한 인코딩 값인 self.input_ch_views이 concat 되어있다.\n",
    "        # 따라서 우선 이걸 torch.split를 dim = -1 (마지막 dim)에서 나눠준다.\n",
    "        input_pts, input_views = torch.split(x, [self.input_ch, self.input_ch_views], dim=-1) # (B, 63), (B, 27)\n",
    "        \n",
    "        h = input_pts\n",
    "        # 인덱스 4 레이어에서 h를 한 번 더 concat 하니까 인덱스를 알기 위해 enum 사용\n",
    "        for i, l in enumerate(self.pts_linears):\n",
    "            h = self.pts_linears[i](h) # i번째 레이어에 h를 넣고\n",
    "            h = F.relu(h) # relu 태워주기\n",
    "            if i in self.skips: # 인덱스가 self.skips에 포함된다면\n",
    "                h = torch.cat([input_pts, h], -1) # (B, 63+256)\n",
    "        # 현재 h: 위치 자표에 대한 W 차원의 output\n",
    "\n",
    "        # view dirs의 사용 여부(논문에선 사용)\n",
    "        if self.use_viewdirs:\n",
    "            alpha = self.alpha_linear(h) # for loop를 다 돌고 나온 h에 self.alpha_linear를 통과시켜 투명도인 alpha를 생성\n",
    "            # h를 다시 feature layer에 통과시켜 변형\n",
    "            feature = self.feature_linear(h) # for loop를 다 돌고 나온 h에 self.feature_linear를 통과시켜 feature를 생성\n",
    "            h = torch.cat([feature, input_views], -1) # feature_linear를 input_views와 concat\n",
    "            # 위치와 방향 두 가지 정보를 조합해서 rgb 컬러 값을 낸다.\n",
    "            # 보는 방향이 달라졌을 때, 빛의 반사 등에 의해 같은 지점의 색상도 다르게 보일 수 있으니 방향 정보를 concat 하는 것\n",
    "\n",
    "            # views_linears를 거쳐 최종 rgb값 산출\n",
    "            for i, l in enumerate(self.views_linears):\n",
    "                h = self.views_linears[i](h)\n",
    "                h = F.relu(h)\n",
    "\n",
    "            # 마지막 rgb 레이어 (256 to 128)\n",
    "            rgb = self.rgb_linear(h)\n",
    "            outputs = torch.cat([rgb, alpha], -1)\n",
    "        else:\n",
    "            outputs = self.output_linear(h) # view dir를 사용하지 않을 경우 단일 레이어에서 rgb, alpha 모두 얻음.\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3163, 0.6308, 0.3783]])\n",
      "tensor([[0.1616, 0.4788, 0.7762]])\n",
      "torch.Size([1, 63])\n",
      "torch.Size([1, 27])\n",
      "torch.Size([1, 90])\n",
      "torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "# 인풋 값 생성\n",
    "input = torch.rand(1,3) # 배치 1, input은 x, y, z로 3\n",
    "input_dir = torch.rand(1,3) # 배치 1, dir도 3\n",
    "print(input)\n",
    "print(input_dir)\n",
    "\n",
    "# 임베딩 하기\n",
    "embed, out_dim = get_embedder(10) # 좌표 옥타브는 10\n",
    "embed_dir, out_dim_dir = get_embedder(4) # 방향 옥타브는 4\n",
    "\n",
    "# 임베딩한 결과값\n",
    "emb_input = embed(input)\n",
    "emb_input_dir = embed_dir(input_dir)\n",
    "print(emb_input.shape)\n",
    "print(emb_input_dir.shape)\n",
    "\n",
    "# 모델 생성\n",
    "model = NeRF(input_ch=out_dim, input_ch_views=out_dim_dir, use_viewdirs=True)\n",
    "x = torch.cat([emb_input, emb_input_dir], dim=-1)\n",
    "print(x.shape)\n",
    "out = model(x)\n",
    "\n",
    "print(out.shape) # out은 rgb와 알파 값이 concat 되어 나온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "e1qslEhDj5ER"
   },
   "outputs": [],
   "source": [
    "# 함수를 return 하는 함수\n",
    "# chunk: flattend 된 N_rays * N_samples이 매우 큰 경우 (512*512*64) 이 배치가 연산이 안 될 수 있다.\n",
    "# 따라서 모델이 처리할 수 있는 max 사이즈를 chunk 단위로 끊어서 연산한다.\n",
    "def batchify(fn, chunk): # fn은 분할하여 연산할 함수. 여기선 NeRF 모델.\n",
    "    \"\"\"Constructs a version of 'fn' that applies to smaller batches.\n",
    "    \"\"\"\n",
    "    if chunk is None: # chunk: 한 번에 처리할 수 있는 최대 샘플 수(배치 크기).\n",
    "        return fn # fn: 실제 모델(NeRF MLP) 등, 텐서를 입력받아 출력 텐서를 반환하는 함수.\n",
    "    # 만약 chunk가 None이면, 그냥 fn을 그대로 반환(즉, 분할 없음).\n",
    "    # 그렇지 않다면, 내부에 ret(inputs)라는 새 함수를 정의 → 이 함수가 실제로 “batchify”를 수행.\n",
    "    def ret(inputs):\n",
    "        # chunk 단위로 잘라서 하는 것\n",
    "        # inputs.shape[0]은 (N_rays × N_samples)와 같은 0번 축 크기.\n",
    "        # range(0, inputs.shape[0], chunk)로 0번 축(첫 번째 차원)을 chunk 단위씩 나눈다.\n",
    "        # 예: chunk=4096이면, inputs[0:4096], inputs[4096:8192], 등으로 슬라이스.\n",
    "        # 각 조각을 fn(...)에 넣어 계산 → 결과 리스트를 torch.cat(..., dim=0)으로 연결.\n",
    "        # 최종적으로 모든 조각이 합쳐진 결과를 반환.\n",
    "        return torch.cat([fn(inputs[i:i+chunk]) for i in range(0, inputs.shape[0], chunk)], 0) # 0부터 N_rays * N_samples까지 chunk 단위로 잘라 nerf에 통과시킨 뒤, 이들을 전부 concat하여 return\n",
    "        # 즉, (N_rays×N_samples, D) 형태로 flatten된 대규모 입력을,\n",
    "        # (chunk, D) 단위로 여러 번 나눠서 모델에 넣고,\n",
    "        # 그 출력을 이어붙이는 역할을 함.\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Qi03XbugjOgI"
   },
   "outputs": [],
   "source": [
    "# 모델을 실행하는 code\n",
    "# inputs, viewdirs, fn, embed_fn, embeddirs_fn를 받아 포지션 임베딩부터 모델 통과까지를 일괄 처리 해주는 함수\n",
    "def run_network(inputs, viewdirs, fn, embed_fn, embeddirs_fn, netchunk=1024*64):\n",
    "    # inputs: shape [N_rays, N_samples, 3] (ex: [H×W, N, 3]), 즉 광선 × 샘플 포인트들.\n",
    "    # viewdirs: shape [N_rays, 3] (광선마다 동일 방향), 있을 수도 없을 수도.\n",
    "    # fn: NeRF 모델 함수(예: Coarse/Fine MLP)\n",
    "    # embed_fn, embeddirs_fn: 각각 위치·방향에 대한 Positional Encoding 함수.\n",
    "    # netchunk: 한 번에 처리 가능한 chunk 크기(디폴트 65536).\n",
    "\n",
    "    inputs_flat = torch.reshape(inputs, [-1, inputs.shape[-1]]) # (H×W×N, 3)= (N_rays * N_samples, 3)로 펼침.\n",
    "    embedded = embed_fn(inputs_flat) # 포지션 인코딩 (H×W×N, 63).\n",
    "\n",
    "    if viewdirs is not None:\n",
    "        # viewdirs [N_rays, 3] 하나의 ray 상의 dir는 동일하니 N_samples 할 필요가 없다.\n",
    "        # 따라서 같은 모델에 넣기 위해 dim을 추가해줘야 한다.\n",
    "\n",
    "        # viewdirs[:, None]: [N_rays, 1, 3]\n",
    "        # viewdirs[:, None].expand(inputs.shape): [N_rays, N_samples, 3]\n",
    "        input_dirs = viewdirs[:, None].expand(inputs.shape)   # input_dirs = (N_rays, N_samples, 3)\n",
    "        input_dirs_flat = torch.reshape(input_dirs, [-1, input_dirs.shape[-1]]) # [N_rays * N_samples, 3]\n",
    "        embedded_dirs = embeddirs_fn(input_dirs_flat) # 포지션 인코딩 (H×W×N, 27).\n",
    "\n",
    "        embedded = torch.cat([embedded, embedded_dirs], -1) # concat 진행. 최종 (H×W×N, 63 + 27)\n",
    "\n",
    "    outputs_flat = batchify(fn, netchunk)(embedded) # run network\n",
    "    # list(inputs.shape[:-1]): 마지막 채널 전까지, 즉 [N_rays, N_samples]의 모양으로 다시 펼치고, 마지막 4차원인 [outputs_flat.shape[-1]의 모양으로 reshape\n",
    "    outputs = torch.reshape(outputs_flat, list(inputs.shape[:-1]) + [outputs_flat.shape[-1]]) # [N_rays, N_samples, output_ch=4]\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) chunk는 한 광선의 전체 샘플 수(N)의 배수여야 하는가?\n",
    "\n",
    "결론부터 말하면, **반드시 배수일 필요는 없습니다.**  \n",
    "각 광선(ray)에 대해 샘플링한 점들이 chunk 경계에서 “중간에 잘려” 들어가도, **모델(MLP) 입장에서는 아무 문제**가 없습니다.\n",
    "\n",
    "### 왜 괜찮을까?\n",
    "\n",
    "- **NeRF 신경망(MLP)은 완전히 점별(point-wise) 독립적인 함수**입니다.  \n",
    "  - (x,y,z) 한 점이 입력으로 들어오면, (density, color) 하나를 출력할 뿐, “ray의 다른 점들”과 시간을 공유하거나 RNN처럼 상태를 공유하지 않습니다.\n",
    "- **Chunk**는 단지 “(N_rays × N_samples)이 너무 커서 한 번에 GPU에 못 태우는 경우”에, 0번 축으로 슬라이스해 **여러 번** 나눠서 연산할 뿐입니다.  \n",
    "  - 즉, $\\text{flatten}$된 1D 인덱스 순서대로 잘라서 MLP에 넣고, 결과를 이어 붙이면, 최종 순서가 동일하게 복원되므로 문제가 없습니다.\n",
    "- NeRF 후처리(볼륨 렌더링 등)에서는 “각 레이의 $\\alpha$, color”를 **다시 같은 인덱스 순**으로 `reshape`해서 쓰기 때문에, 한 레이의 5개 점이 chunk 경계를 사이에 두고 분산되어 처리되어도 괜찮습니다.\n",
    "\n",
    "\n",
    "예를 들어, 하나의 광선에서 5개의 점($p_0, p_1, p_2, p_3, p_4$)을 샘플링 했다고 가정했을 때,\n",
    "\n",
    "$$\n",
    "\\text{MLP}(p_0), \\text{MLP}(p_1), \\ldots \\text{MLP}(p_4)\n",
    "$$\n",
    "이 한꺼번에 들어가든,  \n",
    "$$\n",
    "\\text{MLP}(p_0), \\text{MLP}(p_1), \\text{MLP}(p_2)\\quad \\text{와}\\quad \\text{MLP}(p_3), \\text{MLP}(p_4)\n",
    "$$\n",
    "로 두 번 나눠 들어가든, 결과적으로는 모델이 “입력-출력 대응”만 하면 되므로 **학습 혼동**은 없습니다.\n",
    "\n",
    "\n",
    "### 2) 실제 `torch.reshape()`함수가 어떻게 동작하는가?\n",
    "\n",
    "- **가정**: `H=4, W=6`, 총 24픽셀인 화면에서, 각 픽셀마다 `3`차원 좌표 (x,y,z)로 이루어진 샘플 포인트를 `N=5`개씩 뽑는다고 가정하자.   \n",
    "- 그럼 `inputs.shape = (24, 5, 3)`이다.\n",
    "- 또한 `inputs[i, j]` = i번째 픽셀의 j번째 샘플 포인트로 `shape = (3,)`이다.\n",
    "\n",
    "#### (1) flatten될 때, 원소의 순서\n",
    "\n",
    "```python\n",
    "inputs_flat = torch.reshape(inputs, [-1, inputs.shape[-1]])\n",
    "# shape: (24*5, 3) = (120, 3)\n",
    "```\n",
    "\n",
    "PyTorch에서는 **row-major** 방식(“C 스타일”)으로 텐서를 일렬로 펼칩니다. 즉,\n",
    "\n",
    "1. 0번 픽셀 (`i=0`)에 대해 j=0..4 (총 5개 샘플)이 순서대로\n",
    "2. 1번 픽셀 (`i=1`)에 대해 j=0..4\n",
    "3. 2번 픽셀 (`i=2`)에 대해 j=0..4\n",
    "…  \n",
    "5. 23번 픽셀 (`i=23`)에 대해 j=0..4\n",
    "\n",
    "이 순서로 0번 축이 이어진 뒤, 마지막 축(3)은 그대로 (x,y,z)로 모양을 유지.\n",
    "\n",
    "\n",
    "\n",
    "#### 구체 예시\n",
    "\n",
    "**inputs**  \n",
    "`inputs[0,1,:] = (x00,y00,z00)`  \n",
    "`inputs[0,1,:] = (x01,y01,z01)`  \n",
    "…  \n",
    "`inputs[0,4,:] = (x04,y04,z04)`  \n",
    "\n",
    "**inputs_flat**  \n",
    "`inputs_flat[0] = (x00,y00,z00)`  \n",
    "`inputs_flat[1] = (x01,y01,z01)`  \n",
    "`inputs_flat[2] = (x02,y02,z02)`  \n",
    "`inputs_flat[3] = (x03,y03,z03)`  \n",
    "`inputs_flat[4] = (x04,y04,z04)`  \n",
    "\n",
    "`inputs_flat[5] = (x10,y10,z10)`  \n",
    "`inputs_flat[6] = (x11,y11,z11)`  \n",
    "`inputs_flat[7] = (x12,y12,z12)`  \n",
    "`inputs_flat[8] = (x13,y13,z13)`  \n",
    "`inputs_flat[9] = (x14,y14,z14)`  \n",
    "…\n",
    "\n",
    "이런 식으로 `i` 픽셀마다 5샘플씩 차례대로 나열되어, 총 (120, 3)이 됩니다.\n",
    "\n",
    "#### (2) Chunk가 샘플 포인트의 배수가 아닌 경우는?\n",
    "\n",
    "- 예: `batchify(fn, chunk=3)`라면, `inputs_flat.shape = (120,3)`을  \n",
    "  - 1번째: `inputs_flat[0:3]` \n",
    "  - 2번째: `inputs_flat[3:6]`\n",
    "  - 3번째: `inputs_flat[6:9]`\n",
    "  …  \n",
    "  - 40번째: `inputs_flat[117:120]`\n",
    "\n",
    "- 이렇게 **40번** 나누어 모델에 통과시키고, 각각의 `inputs_flat.shape=(3, 3)`을 0번 차원으로 이어 붙이면 최종 `ret.shape=(120, 3)`이 생성.\n",
    "- 이때 `0번 픽셀의 5개 샘플` 중 **첫 3개는 첫 chunk, 나머지 2개는 두 번째 chunk**에 들어가 처리됩니다.  \n",
    "- 그러나 네트워크는 포인트별 독립 계산이므로 전혀 문제 없음.\n",
    "\n",
    "### 3) 텐서의 차원 추가\n",
    "\n",
    "아래 코드는 **뷰 방향(input_dirs)을** “inputs_flat”에 맞춰 반복해서 붙여주는 전형적인 방식입니다.  \n",
    "다시 말해, 한 광선(ray)에 대해 그 광선 전체가 **동일한 방향**을 갖고 있으므로, 각 샘플 지점마다 동일한 값을 주어 확장하는 것입니다.\n",
    "\n",
    "```python\n",
    "# viewdirs.shape: (N_rays, 3)\n",
    "# viewdirs.shape: (N_rays, 1, 3)\n",
    "input_dirs = viewdirs[:, None].expand(inputs.shape)   # (N_rays, N_samples, 3)\n",
    "input_dirs_flat = torch.reshape(input_dirs, [-1, input_dirs.shape[-1]]) # (N_rays*N_samples, 3)\n",
    "```\n",
    "\n",
    "#### (1) 전제: `H=4, W=6` → $4\\times6=24$개의 광선, 각 광선마다 `N_samples=5`\n",
    "\n",
    "- **`viewdirs.shape = (24,3)`**:  \n",
    "  - 광선이 24개이고, 각 광선의 3차원 방향벡터를 저장.  \n",
    "  - 예를 들어,  \n",
    "  `viewdirs[0]=(x0,y0,z0)`  \n",
    "  `viewdirs[1]=(x1,y1,z1)`  \n",
    "  …  \n",
    "  `viewdirs[23]=(x23,y23,z23)`  \n",
    "\n",
    "- **`inputs.shape = (24,5,3)`**:  \n",
    "  - 24개 광선 각각에 대해, 샘플 포인트가 5개씩 존재.  \n",
    "  - 예: `inputs[i, j]` = $i$번째 광선의 $j$번째 샘플 좌표.\n",
    "\n",
    "- 그러나 $i$번째 광선에 있는 모든 샘플의 방향은 동일하므로, **viewdirs**의 방향 벡터를 **N_samples** 만큼 확장해줘야 한다.\n",
    "\n",
    "#### (2) `viewdirs[:, None]` → shape `(24, 1, 3)`\n",
    "\n",
    "1. **원본**: viewdirs.shape = (24, 3)  \n",
    "   $$\n",
    "     \\begin{bmatrix}\n",
    "     x_0 & y_0 & z_0 \\\\\n",
    "     x_1 & y_1 & z_1 \\\\\n",
    "     \\vdots & \\vdots & \\vdots \\\\\n",
    "     x_{23} & y_{23} & z_{23}\n",
    "     \\end{bmatrix}\n",
    "     \\quad(\\text{24, 3})\n",
    "   $$\n",
    "2. **`[:, None]`** (또는 `.unsqueeze(1)`):  \n",
    "두 번째 축에 길이 1을 추가해, shape `(24, 1, 3)`로 바뀜.\n",
    "시각적으로, 이제 각 행이 “(1,3)짜리 소그룹”이 됩니다.\n",
    "\n",
    "$$\n",
    "\\text{viewdirs[:, None]} = \n",
    "\\begin{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_0 & y_0 & z_0\n",
    "\\end{bmatrix}\n",
    "\\\\[4pt]\n",
    "\\begin{bmatrix}\n",
    "x_1 & y_1 & z_1\n",
    "\\end{bmatrix}\n",
    "\\\\[2pt]\n",
    "\\vdots \\\\\n",
    "\\begin{bmatrix}\n",
    "x_{23} & y_{23} & z_{23}\n",
    "\\end{bmatrix}\n",
    "\\end{bmatrix}\n",
    "\\quad\\text{(24,1,3)}\n",
    "$$\n",
    "이를 3차원 텐서로 보면, 첫 번째 축(=24)은 “각 광선 index”, 두 번째 축(=1)은 “길이가 1인 차원”, 마지막 축(=3)은 xyz 벡터.\n",
    "\n",
    "#### (3) `.expand(inputs.shape) → Shape (24, 5, 3)`\n",
    "\n",
    "`inputs.shape`는 `(24,5,3)`.  \n",
    "따라서 **`viewdirs[:, None].expand(inputs.shape)`**는 (24,1,3)의 두 번째 축(=1)을 5차원으로 **브로드캐스트**(복제)하여 `(24,5,3)`를 만듭니다.\n",
    "\n",
    "#### 어떤 값이 들어갈까?\n",
    "\n",
    "- $i$번째 광선의 방향벡터가, 그 광선의 모든 5개의 샘플에 걸쳐 동일하게 **반복**됩니다.  \n",
    "- 예를 들어, `i=0` 광선이면, shape `(1,3)` -> shape `(5,3)`로 5번 복제  \n",
    "  $$\n",
    "    \\begin{bmatrix}\n",
    "    x_0 & y_0 & z_0\\\\\n",
    "    x_0 & y_0 & z_0\\\\\n",
    "    x_0 & y_0 & z_0\\\\\n",
    "    x_0 & y_0 & z_0\\\\\n",
    "    x_0 & y_0 & z_0\n",
    "    \\end{bmatrix}\n",
    "  $$\n",
    "- 전체로는 `(24,5,3)` 텐서가 되며, 각 [i, s, :] = $(x_i, y_i, z_i)$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        x_0 & y_0 & z_0 \\\\\n",
    "        x_0 & y_0 & z_0 \\\\\n",
    "        x_0 & y_0 & z_0 \\\\\n",
    "        x_0 & y_0 & z_0 \\\\\n",
    "        x_0 & y_0 & z_0\n",
    "    \\end{bmatrix},\n",
    "    &\n",
    "    \\begin{bmatrix}\n",
    "        x_1 & y_1 & z_1 \\\\\n",
    "        x_1 & y_1 & z_1 \\\\\n",
    "        x_1 & y_1 & z_1 \\\\\n",
    "        x_1 & y_1 & z_1 \\\\\n",
    "        x_1 & y_1 & z_1\n",
    "    \\end{bmatrix},\n",
    "    &\n",
    "    \\cdots,\n",
    "    &\n",
    "    \\begin{bmatrix}\n",
    "        x_{23} & y_{23} & z_{23} \\\\\n",
    "        x_{23} & y_{23} & z_{23} \\\\\n",
    "        x_{23} & y_{23} & z_{23} \\\\\n",
    "        x_{23} & y_{23} & z_{23} \\\\\n",
    "        x_{23} & y_{23} & z_{23}\n",
    "    \\end{bmatrix}\n",
    "\\end{bmatrix}\n",
    "\\quad\\text{(24,5,3)}\n",
    "$$\n",
    "\n",
    "  \n",
    "- 표로 표현하면, 예컨데 (24,5,3)을 2D로 써보면 아래처럼,  \n",
    "각 광선 index i가 행, 각 샘플 s가 열, (x,y,z)는 벡터로\n",
    "\n",
    "    | **광선 $i$** | 샘플 0                | 샘플 1                | 샘플 2                | 샘플 3                | 샘플 4                |\n",
    "    |:-----------:|:--------------------:|:--------------------:|:--------------------:|:--------------------:|:--------------------:|\n",
    "    | **0**     | $(x_0,y_0,z_0)$            | $(x_0,y_0,z_0)$            | $(x_0,y_0,z_0)$            | $(x_0,y_0,z_0)$            | $(x_0,y_0,z_0)$            |\n",
    "    | **1**     | $(x_1,y_1,z_1)$            | $(x_1,y_1,z_1)$            | $(x_1,y_1,z_1)$            | $(x_1,y_1,z_1)$            | $(x_1,y_1,z_1)$            |\n",
    "    | **2**     | $(x_2,y_2,z_2)$            | $(x_2,y_2,z_2)$            | $(x_2,y_2,z_2)$            | $(x_2,y_2,z_2)$            | $(x_2,y_2,z_2)$            |\n",
    "    | ...       |                        |                        |                        |                        |                        |\n",
    "    | **23**    | $(x_{23},y_{23},z_{23})$         | $(x_{23},y_{23},z_{23})$         | $(x_{23},y_{23},z_{23})$         | $(x_{23},y_{23},z_{23})$         | $(x_{23},y_{23},z_{23})$         |\n",
    "\n",
    "즉, 한 광선 $i$에 대한 **단일 방향**이 “해당 광선의 5개 샘플 지점마다 동일”하게 복제되어, 최종 `(24,5,3)` 형태로 담깁니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "c3t0hbiWFrLn"
   },
   "outputs": [],
   "source": [
    "# 학습에 들어갈 모든 것을 생성하는 wrap up 함수\n",
    "def create_nerf(args):\n",
    "    \"\"\"Instantiate NeRF's MLP model.\n",
    "    \"\"\"\n",
    "    # (1) view 위치 임베딩\n",
    "    # args.multires: 옥타브 args.i_embed: 임베딩 여부(당연히 사용)\n",
    "    embed_fn, input_ch = get_embedder(args.multires, args.i_embed) # 10, 0\n",
    "    # embed_fn: 임베딩 함수\n",
    "    # iput_ch: 임베딩 후 차원(63)\n",
    "\n",
    "    # 변수 초기화\n",
    "    input_ch_views = 0\n",
    "    embeddirs_fn = None\n",
    "\n",
    "    # (2) view dir 임베딩\n",
    "    if args.use_viewdirs:\n",
    "        embeddirs_fn, input_ch_views = get_embedder(args.multires_views, args.i_embed)\n",
    "    # embeddirs_fn: 임베딩 함수\n",
    "    # input_ch_views: 임베딩 후 차원(27)\n",
    "\n",
    "    # (3) coarse 모델 생성\n",
    "    # 여기서 웨이트는 transmitance와 occupancy의 곱 (수식 참조)\n",
    "\n",
    "    # view_dirs를 사용하지 않을 경우 output_ch = 5의 결과를 반환.\n",
    "    # 그러나 우리는 veiew_dirs를 사용하므로 이 세팅은 중요하지 않음.\n",
    "    output_ch = 5 if args.N_importance > 0 else 4\n",
    "    \n",
    "    skips = [4]\n",
    "    # coarse 모델을 생성\n",
    "    model = NeRF(D=args.netdepth, W=args.netwidth,\n",
    "                 input_ch=input_ch, output_ch=output_ch, skips=skips,\n",
    "                 input_ch_views=input_ch_views, use_viewdirs=args.use_viewdirs).to(device)\n",
    "    \n",
    "    # 모델 파라미터를 학습할 파라미터 리스트에 저장\n",
    "    grad_vars = list(model.parameters())\n",
    "\n",
    "    # coase 모델의 weight가 높은 곳에서는 중요도가 높기 때문에 더 많은 점을 샘플링\n",
    "    # 내부 공간에 대한 샘플링은 덜 하게 된다\n",
    "    model_fine = None\n",
    "    if args.N_importance > 0:\n",
    "        model_fine = NeRF(D=args.netdepth_fine, W=args.netwidth_fine,\n",
    "                          input_ch=input_ch, output_ch=output_ch, skips=skips,\n",
    "                          input_ch_views=input_ch_views, use_viewdirs=args.use_viewdirs).to(device)\n",
    "        \n",
    "        grad_vars += list(model_fine.parameters()) # 모델 파라미터 append\n",
    "\n",
    "    # 학습에 필요한 모든 요소를 wrap up하여 준비해주는 함수\n",
    "    # 밑에 render_kwargs_train에 input data로 들어감\n",
    "    network_query_fn = lambda inputs, viewdirs, network_fn : run_network(inputs, viewdirs, network_fn,\n",
    "                                                                 embed_fn=embed_fn,\n",
    "                                                                 embeddirs_fn=embeddirs_fn,\n",
    "                                                                 netchunk=args.netchunk)\n",
    "\n",
    "    # Create optimizer\n",
    "    # grad_vars: 모델 파라미터 리스트\n",
    "    optimizer = torch.optim.Adam(params=grad_vars, lr=args.lrate, betas=(0.9, 0.999))\n",
    "\n",
    "    start = 0\n",
    "    basedir = args.basedir\n",
    "    expname = args.expname\n",
    "\n",
    "    ##########################\n",
    "\n",
    "    # Load checkpoints\n",
    "    if args.ft_path is not None and args.ft_path!='None':\n",
    "        ckpts = [args.ft_path]\n",
    "    else:\n",
    "        ckpts = [os.path.join(basedir, expname, f) for f in sorted(os.listdir(os.path.join(basedir, expname))) if 'tar' in f]\n",
    "\n",
    "    print('Found ckpts', ckpts)\n",
    "    if len(ckpts) > 0 and not args.no_reload:\n",
    "        ckpt_path = ckpts[-1]\n",
    "        print('Reloading from', ckpt_path)\n",
    "        ckpt = torch.load(ckpt_path)\n",
    "\n",
    "        start = ckpt['global_step']\n",
    "        optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
    "\n",
    "        # Load model\n",
    "        model.load_state_dict(ckpt['network_fn_state_dict']) # coarse model\n",
    "        if model_fine is not None:\n",
    "            model_fine.load_state_dict(ckpt['network_fine_state_dict']) # fine model\n",
    "\n",
    "    ##########################\n",
    "\n",
    "    # 학습에 대한 딕셔너리를 하나 선언해서 나중에 학습할 때 input으로 쓰도록.\n",
    "    render_kwargs_train = {\n",
    "        'network_query_fn' : network_query_fn, # 위에서 생성한 네트워크 쿼리\n",
    "        'perturb' : args.perturb,\n",
    "        'N_importance' : args.N_importance, # fine 모델이 샘플할 개수(128)\n",
    "        'network_fine' : model_fine,\n",
    "        'N_samples' : args.N_samples, # coarse 모델이 샘플할 개수(64)\n",
    "        'network_fn' : model,\n",
    "        'use_viewdirs' : args.use_viewdirs, # True\n",
    "        'white_bkgd' : args.white_bkgd, # 배경을 흰색으로 할 것인가?\n",
    "        'raw_noise_std' : args.raw_noise_std, # 노이즈를 섞어서 오버피팅 막기\n",
    "    }\n",
    "\n",
    "    # NDC only good for LLFF-style forward facing data\n",
    "    if args.dataset_type != 'llff' or args.no_ndc:\n",
    "        print('Not ndc!')\n",
    "        render_kwargs_train['ndc'] = False\n",
    "        render_kwargs_train['lindisp'] = args.lindisp\n",
    "\n",
    "    # test 시에는 모델의 perturb와 noise를 없애준다.\n",
    "    render_kwargs_test = {k : render_kwargs_train[k] for k in render_kwargs_train}\n",
    "    render_kwargs_test['perturb'] = False\n",
    "    render_kwargs_test['raw_noise_std'] = 0.\n",
    "\n",
    "    # start는 훈련 스텝\n",
    "    return render_kwargs_train, render_kwargs_test, start, grad_vars, optimizer\n",
    "\n",
    "    # render_kwargs_train: 학습/렌더링 중에 사용할 파라미터들(모델, 샘플수, etc.).\n",
    "    # render_kwargs_test: 테스트/렌더 시 쓸 파라미터(노이즈/perturb 꺼짐).\n",
    "    # start: 로딩된 global_step(ckpt), 없으면 0.\n",
    "    # grad_vars: 모델 파라미터 목록.\n",
    "    # optimizer: Adam 최적화 인스턴스.\n",
    "    # 이렇게 하면, 학습 루프에서:\n",
    "\n",
    "    # optimizer.zero_grad()\n",
    "    # 광선 계산 → render 함수(=Volumetric rendering) → render_kwargs_train 활용\n",
    "    # 로스 계산 → loss.backward()\n",
    "    # optimizer.step()ㄴ\n",
    "    # 등을 수행해 모델을 훈련할 수 있습니다."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPmQ0ATXgks8InBKxqkje51",
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "white",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
